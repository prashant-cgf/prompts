def task_achieve2(evaluation_text:str):
    print("enterred")
    task=evaluation_text["question_text"]
    Coherence_Cohesion = f"""
        Please evaluate the user_prompt
        response based on the Coherence and Cohesion criteria for the IELTS Writing test.
        Assign a band score from 1 to 9 based on the criteria provided.
        Task objective is {task}
        the user_prompt should be according Task Objective.
        Criteria:
        Band 9:
        All requirements of the task are fully and appropriately satisfied.
        Extremely rare lapses in content.
        
        Band 8:
        The response covers all requirements of the task appropriately, relevantly, and sufficiently.
        Key features are skillfully selected and clearly presented, highlighted, and illustrated.
        Occasional omissions or lapses in content.
        
        Band 7:
        The response covers the requirements of the task.
        Content is relevant and accurate with few omissions or lapses.
        Format is appropriate.
        Clear overview, data are appropriately categorized, main trends or differences are identified.
        Minimal lapses.
        
        Band 6:
        Focus on the requirements of the task.
        Appropriate format.
        Key features are adequately highlighted, a relevant overview is attempted, information is appropriately selected and supported using figures/data.
        Some irrelevant, inappropriate, or inaccurate information.
        Some details may be missing or excessive.
        
        Band 5:
        The response generally addresses the requirements of the task.
        The format may be inappropriate in places.
        Key features are not adequately covered.
        Mainly mechanical recounting of detail, no data support.
        Some bullet points may not be adequately covered.
        Variable and sometimes inappropriate tone.
        Limited detail when extending and illustrating main points.
        
        Band 4:
        Attempt to address the task.
        Few key features selected.
        Not all bullet points are presented.
        Purpose of the letter is not clearly explained and may be confused.
        Irrelevant, repetitive, inaccurate, or inappropriate content.
        
        Band 3:
        The response does not address the requirements of the task.
        Misunderstanding of the data/diagram/situation.
        Limited, repetitive information.
        
        Band 2:
        The content barely relates to the task.
        
        Band 1:
        Responses of 20 words or fewer.
        Task response is wholly unrelated to the question.
        
        Band 0:
        Should  be used where Task response is blank or single line  or the response is not a proper sentence in english"""
    def llm(system_prompt: str, user_prompt: str) -> str:
            # ensure your LLM imports are all within this function
            
            
        # define your own LLM here
        client = OpenAI()
        response = client.chat.completions.create(
            model='gpt-4o',
            temperature = 0.8,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
        )
        return response.choices[0].message.content
    
    res = strict_json(system_prompt = Coherence_Cohesion,
                      user_prompt=evaluation_text["answer_text"],
                        output_format = {'score': "Avg. score round to integer for below socores in reason" , 
                                         "comments":"brief description why this score is given",
                                         'reason': [
                                             {
                                                "complete_response": {
                                                    "score": "scoring of user_prompt for complete response",
                                                    "explanation": "reason with example statements for this score"
                                                }
                                            },
                                            {
                                                "clear_comprehensive_ideas": {
                                                    "score": "scoring of user_prompt for clear and comprehensive ideas",
                                                    "explanation": "reason with example statements for this score"
                                                }
                                            },
                                            {
                                                "appropriate_word_count": {
                                                    "score": "scoring of user_prompt for appropriate_word_count",
                                                    "explanation": "reason with example statements for this score"
                                                }
                                            }
                                             ],
                                        'suggestion': 'How to improve score with example statements'},
                                         llm = llm)
    print("----------------------------------------------------------------------------------------------------------------------")                 
    print("res---------->>>>>>>>>>>",res)
    comment=""
    try:
        comment=res["comments"]
        del res["comments"]
    except:
        pass    
    return res,comment

def Coherance2(evaluation_text:str):
    print("enterred")
    task=evaluation_text["question_text"]
    Coherence_Cohesion = f"""
        Please evaluate the user_prompt
        response based on the Coherence and Cohesion criteria for the IELTS Writing test.
        Assign a band score from 1 to 9 based on the criteria provided.
        Task objective is {task}
        the user_prompt should be according Task Objective.
        Criteria:
        Band,Description
        9,"The message can be followed effortlessly. Cohesion is used in such a way that it very rarely attracts attention. Any lapses in coherence or cohesion are minimal. Paragraphing is skillfully managed."
        8,"The message can be followed with ease. Information and ideas are logically sequenced, and cohesion is well managed. Occasional lapses in coherence or cohesion may occur. Paragraphing is used sufficiently and appropriately."
        7,"Information and ideas are logically organized and there is a clear progression throughout the response. A few lapses may occur. A range of cohesive devices including reference and substitution is used flexibly but with some inaccuracies or some over/under use."
        6,"Information and ideas are generally arranged coherently and there is a clear overall progression. Cohesive devices are used to some good effect but cohesion within and/or between sentences may be faulty or mechanical due to misuse, overuse or omission. The use of reference and substitution may lack flexibility or clarity and result in some repetition or error."
        5,"Organisation is evident but is not wholly logical and there may be a lack of overall progression. Nevertheless, there is a sense of underlying coherence to the response. The relationship of ideas can be followed but the sentences are not fluently linked to each other. There may be limited/overuse of cohesive devices with some inaccuracy. The writing may be repetitive due to inadequate and/or inaccurate use of reference and substitution."
        4,"Information and ideas are evident but not arranged coherently, and there is no clear progression within the response. Relationships between ideas can be unclear and/or inadequately marked. There is some use of basic cohesive devices, which may be inaccurate or repetitive. There is inaccurate use or a lack of substitution or referencing."
        3,"There is no apparent logical organisation. Ideas are discernible but difficult to relate to each other. Minimal use of sequencers or cohesive devices. Those used do not necessarily indicate a logical relationship between ideas. There is difficulty in identifying referencing."
        2,"There is little relevant message, or the entire response may be off-topic. There is little evidence of control of organisational features."
        1,"Responses of 20 words or fewer. The writing fails to communicate any message and appears to be by a virtual non-writer."
        0,"Should only be used where a Task response is empty or doesnot make any correct sentence or unrelated to task"
        Evaluate the below Task_response assign mimimum scores if Task response is not in context with question, be strict
        Please provide the band score along with a brief justification"""
    def llm(system_prompt: str, user_prompt: str) -> str:
        # ensure your LLM imports are all within this function
        from openai import OpenAI
        
        # define your own LLM here
        client = OpenAI()
        response = client.chat.completions.create(
            model='gpt-4o',
            temperature = 0.8,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
        )
        return response.choices[0].message.content
    
    res = strict_json(system_prompt = Coherence_Cohesion,
                      user_prompt=evaluation_text["answer_text"],
                        output_format = {'score': "Avg. score round to integer for below socores in reason" , 
                                         "comments":"brief description why this score is given",
                                         'reason': [
                                             {'logical_structure': 
                                              {'score': "score for Logical structure ", 
                                               'explanation': "reason for the above score with examples if any",
                                              }},
                                             {'introduction_&_conclusion_present':
                                                  {'score':'score for introduction_&_conclusion_present',
                                                   'explanation': 'reason for giving this score'
                                                  }}, 
                                             {'supported_main_points': 
                                                  {'score': "score after evaluating whether user_answer supported_main_points", 
                                                   'explanation': "reason for giving this score",
                                                  }}, 
                                                 {'accurate_linking_words': 
                                                  {'score': "score after evaluating whether user_answer accurate_linking_word", 
                                                   'explanation': 'reason for giving this score'}}, 
                                                 {'variety_in_linking_words': 
                                                  {'score': "score after evaluating whether user_prompt has variety in linking words", 
                                                   'explanation': "reason for giving this score"}}],
                                        'suggestion': 'How to improve score with example statements'},
                     llm = llm)
    print("----------------------------------------------------------------------------------------------------------------------")                 
    print("res---------->>>>>>>>>>>",res)
    comment=""
    try:
        comment=res["comments"]
        del res["comments"]
    except:
        pass    
    return res,comment

def task_response2(evaluation_text:str):
    print("enterred")
    task=evaluation_text["question_text"]
    Coherence_Cohesion = f"""
        Please evaluate the user_prompt
        response based on the Coherence and Cohesion criteria for the IELTS Writing test.
        Assign a band score from 1 to 9 based on the criteria provided.
        Task objective is {task}
        the user_prompt should be according Task Objective.
        Criteria:
         Band Score	Task Response
            9	All parts of the task are fully and appropriately addressed. There may be extremely rare lapses in content.
            8	All parts of the task are adequately addressed. The position is clear throughout the response. The response is relevant and well-developed. Occasional lapses may occur.
            7	All parts of the task are addressed. A clear position is presented throughout the response. The response is relevant and well-developed with clear ideas supported by examples.
            6	All parts of the task are addressed. The response is mostly relevant and coherent. The main ideas are presented but may be inadequately developed or unclear. Examples are used to some good effect but may lack relevance or sufficiency.
            5	Generally addresses the task but may miss some parts or aspects. The position may be unclear. The main ideas are apparent but not sufficiently developed. There may be some irrelevant or unclear parts.
            4	Attempts to address the task but may miss some key points. The position may be unclear. The ideas are limited and lack development. Examples are irrelevant, inappropriate, or insufficient.
            3	Does not adequately address the task. The position is unclear. Ideas are very limited and not developed. Examples are irrelevant, inappropriate, or absent.
            2	Barely addresses the task. The position is unclear. Ideas are very limited, and there is little development.
            1	Responses of 20 words or fewer are rated at Band 1 also if The content is wholly unrelated to the task.
            0	Should only be where Task response is blank , used a language other than English throughout.
            """
    def llm(system_prompt: str, user_prompt: str) -> str:
            # ensure your LLM imports are all within this function
            
            
        # define your own LLM here
        client = OpenAI()
        response = client.chat.completions.create(
            model='gpt-4o',
            temperature = 0.8,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
        )
        return response.choices[0].message.content

 res = strict_json(system_prompt = Coherence_Cohesion,
                      user_prompt=evaluation_text["answer_text"],
                        output_format = {'score': "Avg. score round to integer for below socores in reason" , 
                                         "comments":"brief description why this score is given",
                                         'reason': [
                                             {
                                                "complete_response": {
                                                    "score": "scoring of user_prompt for complete response",
                                                    "explanation": "reason with example statements for this score"
                                                }
                                            },
                                            {
                                                "clear_comprehensive_ideas": {
                                                    "score": "scoring of user_prompt for clear and comprehensive ideas",
                                                    "explanation": "reason with example statements for this score"
                                                }
                                            },
                                            {
                                                "appropriate_word_count": {
                                                    "score": "scoring of user_prompt for appropriate_word_count",
                                                    "explanation": "reason with example statements for this score"
                                                }
                                            }
                                             ],
                                        'suggestion': 'How to improve score with example statements'},
                                         llm = llm)
    print("----------------------------------------------------------------------------------------------------------------------")                 
    print("res---------->>>>>>>>>>>",res)
    comment=""
    try:
        comment=res["comments"]
        del res["comments"]
    except:
        pass    
    return res,comment

def lexical2(evaluation_text:str):
    task=evaluation_text["question_text"]
    Coherence_Cohesion = f"""
        Please evaluate the user_prompt
        response based on the Coherence and Cohesion criteria for the IELTS Writing test.
        Assign a band score from 1 to 9 based on the criteria provided.
        Task objective is {task}
        the user_prompt should be according Task Objective.
        Criteria:
        Band 9:
        Full flexibility and precise use are evident within the scope of the task.
        A wide range of vocabulary is used accurately and appropriately with very natural and sophisticated control of lexical features.
        Minor errors in spelling and word formation are extremely rare and have minimal impact on communication.
        Band 8:
        A wide resource is fluently and flexibly used to convey precise meanings within the scope of the task.
        There is skilful use of uncommon and/or idiomatic items when appropriate, despite occasional inaccuracies in word choice and collocation.
        Occasional errors in spelling and/or word formation may occur, but have minimal impact on communication.
        Band 7:
        The resource is sufficient to allow some flexibility and precision.
        There is some ability to use less common and/or idiomatic items.
        An awareness of style and collocation is evident, though inappropriacies occur.
        There are only a few errors in spelling and/or word formation, and they do not detract from overall clarity.
        Band 6:
        The resource is generally adequate and appropriate for the task.
        The meaning is generally clear in spite of a rather restricted range or a lack of precision in word choice.
        If the writer is a risk-taker, there will be a wider range of vocabulary used but higher degrees of inaccuracy or inappropriacy.
        There are some errors in spelling and/or word formation, but these do not impede communication.
        Band 5:
        The resource is limited but minimally adequate for the task.
        Simple vocabulary may be used accurately but the range does not permit much variation in expression.
        There may be frequent lapses in the appropriacy of word choice, and a lack of flexibility is apparent in frequent simplifications and/or repetitions.
        Errors in spelling and/or word formation may be noticeable and may cause some difficulty for the reader.
        Band 4:
        The resource is limited and inadequate for or unrelated to the task. Vocabulary is basic and may be used repetitively.
        There may be inappropriate use of lexical chunks (e.g. memorised phrases, formulaic language and/or language from the input material).
        Inappropriate word choice and/or errors in word formation and/or in spelling may impede meaning.
        Band 3:
        The resource is inadequate (which may be due to the response being significantly underlength).
        Possible over-dependence on input material or memorised language.
        Control of word choice and/or spelling is very limited, and errors predominate. These errors may severely impede meaning.
        Band 2:
        The resource is extremely limited with few recognisable strings, apart from memorised phrases.
        There is no apparent control of word formation and/or spelling.
        Band 1:
        user_prompt length is of 20 words or fewer. 
        Band 0:
        user_prompt is of single word.
        
        """
    def llm(system_prompt: str, user_prompt: str) -> str:
        ''' Here, we use OpenAI for illustration, you can change it to your own LLM '''
        # ensure your LLM imports are all within this function
        from openai import OpenAI
        
        # define your own LLM here
        client = OpenAI()
        response = client.chat.completions.create(
            model='gpt-4o',
            temperature = 0.8,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
        )
        return response.choices[0].message.content
    
    res = strict_json(system_prompt = Coherence_Cohesion,
                      user_prompt=evaluation_text["answer_text"],
                        output_format = {'score': "Avg. score round to integer for below scores in reason",
                                         'comments':"Brief explanantion for the scores given",
                                         'reason': [
                                             {
                                                "varied_vocabulary": {
                                                    "score": "score for varied vocabulary i.e evaluation of user_prompt for varied vocabulary",
                                                    "explanation": "reason for the score of varied_vocabulary"
                                                }
                                            },
                                            {
                                                "accurate_spelling_word_formation": {
                                                    "score": "score for accurate spelling word formation i.e evaluation of user_prompt for accurate spelling word formation",
                                                    "explanation": "reason for the score of varied_vocabulary"
                                                }
                                            }
                                         ],
                                        'suggestion': 'How to improve score with example statements'},
                     llm = llm)
    comment=""
    try:
        comment=res["comments"]
        del res["comments"]
    except:
        pass    
    return res,comment

def Grammatical_Range_Accuracy2(evaluation_text:str):
    task=evaluation_text["question_text"]
    Coherence_Cohesion = f"""
        Please evaluate the user_prompt
        response based on the Coherence and Cohesion criteria for the IELTS Writing test.
        Assign a band score from 1 to 9 based on the criteria provided.
        Task objective is {task}
        the user_prompt should be according to Task Objective.
        Criteria:
        Band,Description
        9,"A wide range of structures within the scope of the task is used with full flexibility and control. Punctuation and grammar are used appropriately throughout. Minor errors are extremely rare and have minimal impact on communication."
        8,"A wide range of structures within the scope of the task is flexibly and accurately used. The majority of sentences are error-free, and punctuation is well managed. Occasional, non-systematic errors and inappropriacies occur, but have minimal impact on communication."
        7,"A variety of complex structures is used with some flexibility and accuracy. Grammar and punctuation are generally well controlled, and error-free sentences are frequent. A few errors in grammar may persist, but these do not impede communication."
        6,"A mix of simple and complex sentence forms is used but flexibility is limited. Examples of more complex structures are not marked by the same level of accuracy as in simple structures. Errors in grammar and punctuation occur, but rarely impede communication."
        5,"The range of structures is limited and rather repetitive. Although complex sentences are attempted, they tend to be faulty, and the greatest accuracy is achieved on simple sentences. Grammatical errors may be frequent and cause some difficulty for the reader. Punctuation may be faulty."
        4,"A very limited range of structures is used. Subordinate clauses are rare and simple sentences predominate. Some structures are produced accurately but grammatical errors are frequent and may impede meaning. Punctuation is often faulty or inadequate."
        3,"Sentence forms are attempted, but errors in grammar and punctuation predominate (except in memorised phrases or those taken from the input material). This prevents most meaning from coming through. Length may be insufficient to provide evidence of control of sentence forms."
        2,"There is little or no evidence of sentence forms (except in memorised phrases)."
        1,"Responses of 20 words or fewer. No rateable language is evident.Answer is not in context with question or blank "
        0,"if Answer is not present or single word"
        
        """
    def llm(system_prompt: str, user_prompt: str) -> str:
        client = OpenAI()
        response = client.chat.completions.create(
            model='gpt-4o',
            temperature = 0.8,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
        )
        return response.choices[0].message.content
    
    res = strict_json(system_prompt = Coherence_Cohesion,
                      user_prompt=evaluation_text["answer_text"],
                        output_format = {'score': "Avg. score round to integer for below scores in reason",
                                         'comments':"Brief explanantion for the scores given",
                                         'reason': [
                                             {
                                                  "mix_of_complex_simple_sentences": {
                                                    "score": "score for use of mix of complex and simple sentence in user_prompt",
                                                    "explanation": "reason for giving this score"
                                                  }
                                                },
                                                {
                                                  "clear_and_correct_grammar": {
                                                    "score": "score for use of mix of clear and correct grammar  in the answer i.e user_prompt",
                                                    "explanation": "reason for this scoring"
                                                  }
    }
                                         ],
                                        'suggestion': 'How to improve score with example statements'},
                     llm = llm)
    comment=""
    try:
        comment=res["comments"]
        del res["comments"]
    except:
        pass    
    return res,comment
